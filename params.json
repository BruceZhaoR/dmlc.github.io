{"name":"Distributed Machine Learning Common","tagline":"","body":"### Welcome to DMLC\r\n\r\nDMLC is a community hosting and developing portable, scalable and reliable libraries for distributed machine learning. Contributors come from (in alphabetical order) Baidu, CMU, HKUST, Microsoft, NYU, UW, ... We are looking forward new contributors and projects to join it.\r\n\r\n### Machine Learning Libraries\r\n\r\n- Boosted Trees (GBDT or GBM): [XGBoost](https://github.com/dmlc/xgboost)\r\n- Linear method (Sparse logistic regression, and others..): [SGD](https://github.com/dmlc/wormhole/tree/master/learn/linear), [L-BFGS](https://github.com/dmlc/wormhole/tree/master/learn/lbfgs-linear), [Block CD](https://github.com/dmlc/parameter_server/tree/master/example/linear)\r\n- Deep learning [CXXNET](https://github.com/dmlc/cxxnet) [Minerva](https://github.com/dmlc/minerva)\r\n- Topic Model [LDA](https://github.com/dmlc/lda)\r\n\r\n### System Components\r\n\r\n- I/O (local disk, HDFS, Amazon S3) and Job launchers (Yarn, MPI, ...) [dmlc-core](https://github.com/dmlc/dmlc-core)\r\n- Data communication\r\n    - [rabit](https://github.com/dmlc/rabit) for BSP synchronization(allreduce)\r\n    - [parameter server](https://github.com/dmlc/ps-lite) for asynchronous parameter update\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}